---
title: "Spring AI 模型评估（Model Evaluation）"
date: 2026-02-06
slug: spring-ai-model-evaluation
categories: ["ai"]
tags: ['spring-ai']
---

本文介绍如何在 Spring AI 中使用 **模型评估（Model Evaluation）**：对 LLM 生成的回复做相关性、事实性等评估，常用于 RAG 流程与集成测试，减少幻觉并保证回复与上下文一致。示例 [21-model-evaluation](https://github.com/chensoul/spring-ai-samples/tree/main/21-model-evaluation) 使用 **RelevancyEvaluator** 与 **FactCheckingEvaluator**，通过 **Evaluator** 接口与 **EvaluationRequest/EvaluationResponse** 实现“用模型评模型”。

<!--more-->

> **示例代码库**
>
> 您可以在 [GitHub 仓库](https://github.com/chensoul/spring-ai-samples/tree/main/21-model-evaluation) 中找到本文的示例代码。

## 为什么需要模型评估

LLM 容易产生幻觉或与检索上下文不一致的回复。**模型评估**的做法是：

1. **用另一个（或同一）模型做裁判**：将用户问题、检索到的上下文、模型回复一并交给评估模型，由它判断“回复是否与问题和上下文一致”“回复中的陈述是否被上下文支持”等。
2. **标准化接口**：Spring AI 提供 **Evaluator** 接口，输入 **EvaluationRequest**（userText、dataList 上下文、responseContent），返回 **EvaluationResponse**（pass、feedback、metadata）。
3. **典型用途**：RAG 集成测试、上线前抽样评估、管道中自动过滤低质量回复。

本示例使用 **OpenAI 兼容** Chat 模型（如 DeepSeek）作为评估模型，依赖 `spring-ai-starter-model-openai`，需配置 **DEEPSEEK_API_KEY** 或 **OPENAI_API_KEY**。

## 项目结构概览

- **Application**：Spring Boot 入口。
- **EvaluationConfig**：定义 **RelevancyEvaluator**、**FactCheckingEvaluator** Bean（均基于 **ChatClient.Builder**）。
- **EvaluationController**：`POST /api/evaluate/relevancy`、`POST /api/evaluate/fact-check`，接收 userText、context、responseContent，返回 pass、feedback、metadata。

## 依赖与配置

**pom.xml** 引入 OpenAI 起步依赖（评估器所在 `spring-ai-client-chat` 会随之引入）：

```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-starter-model-openai</artifactId>
</dependency>
```

**application.properties** 中配置评估用 Chat 模型（可与生成回复的模型不同，以控制成本）：

```properties
spring.ai.openai.api-key=${DEEPSEEK_API_KEY:${OPENAI_API_KEY}}
spring.ai.openai.chat.options.model=gpt-4o-mini
```

## Evaluator 接口与实现

**Evaluator** 为函数式接口：

```java
@FunctionalInterface
public interface Evaluator {
    EvaluationResponse evaluate(EvaluationRequest evaluationRequest);
}
```

**EvaluationRequest** 包含：`userText`（用户问题）、`dataList`（上下文，如 RAG 检索到的 `List<Document>`）、`responseContent`（待评估的模型回复）。**EvaluationResponse** 包含：`pass`（是否通过）、`feedback`、`metadata` 等。

### RelevancyEvaluator：相关性评估

判断**回复是否与用户问题和给定上下文一致**，适合验证 RAG 回复是否“扣题且基于检索内容”。

```java
@Bean
RelevancyEvaluator relevancyEvaluator(ChatClient.Builder chatClientBuilder) {
    return new RelevancyEvaluator(chatClientBuilder);
}
```

使用示例：将用户问题、检索到的文档列表、模型回复封装为 **EvaluationRequest**，调用 `relevancyEvaluator.evaluate(request)`，根据 `response.isPass()` 判断是否通过。

### FactCheckingEvaluator：事实核查

判断**回复中的陈述是否被给定上下文支持**，用于减少幻觉（回复中“无依据”的断言会被判为不通过）。

```java
@Bean
FactCheckingEvaluator factCheckingEvaluator(ChatClient.Builder chatClientBuilder) {
    return new FactCheckingEvaluator(chatClientBuilder);
}
```

使用方式同上：同一 **EvaluationRequest** 交给 **FactCheckingEvaluator**，得到 **EvaluationResponse**，根据 `pass` 与 `feedback` 做后续处理。

## 配置与自定义

- 评估模型可与生成模型分离：例如生成用 DeepSeek，评估用 `gpt-4o-mini`，以平衡效果与成本。
- **RelevancyEvaluator** 支持通过 builder 自定义提示模板（需包含 `query`、`response`、`context` 占位符），详见 [Spring AI 文档](https://docs.spring.io/spring-ai/reference/api/testing.html)。

## 运行与测试

**运行应用**（需先设置 API Key）：

```bash
export DEEPSEEK_API_KEY=your-api-key
cd 21-model-evaluation && ../mvnw spring-boot:run
```

**相关性评估示例**：

```bash
curl -s -X POST http://localhost:8080/api/evaluate/relevancy \
  -H "Content-Type: application/json" \
  -d '{
    "userText": "公司年假有多少天？",
    "context": "公司年假制度：入职满一年 5 天，满三年 10 天。",
    "responseContent": "根据制度，入职满一年有 5 天年假。"
  }'
```

**事实核查示例**：

```bash
curl -s -X POST http://localhost:8080/api/evaluate/fact-check \
  -H "Content-Type: application/json" \
  -d '{
    "userText": "年假几天？",
    "context": "年假：满一年 5 天。",
    "responseContent": "年假有 15 天。"
  }'
```

与上下文矛盾的回复在事实核查中应得到 `pass: false`。

## 小结

- Spring AI 通过 **Evaluator**、**EvaluationRequest**、**EvaluationResponse** 提供统一的模型评估抽象。
- **RelevancyEvaluator** 评估回复与问题、上下文的相关性；**FactCheckingEvaluator** 评估回复中的陈述是否被上下文支持，用于减幻觉。
- 评估器内部使用 **ChatClient**（ChatModel），可与生成管道使用不同模型，便于在测试与生产中对 RAG 等流程做自动化质量校验。
